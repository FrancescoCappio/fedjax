{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3aLI4SF1HH1"
      },
      "source": [
        "This tutorial introduces algorithms for federated learning in FedJAX. By completing this tutorial, you'll learn how to write clear and efficient algorithms that follow best practices. It assumes that the readers have finished the tutorial on datasets and models. \n",
        "\n",
        "In order to keep the code pseudo-code-like, we avoid using jax primitives directly while writing algorithms, with the notable exceptions of `jax.random`and `jax.tree_util` libraries. Since lower-level functions that are described in the model tutorial such as `fedjax.optimizers`, `model.grad` are all JIT compiled, the algorithms will still be efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7ByvjrSUPHm"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jax.tree_util as tree_util\n",
        "import numpy as np\n",
        "\n",
        "import fedjax\n",
        "import fedjax.google.datasets as fedjax_data\n",
        "# We only use TensorFlow for datasets, so we restrict it to CPU only to avoid\n",
        "# issues with certain ops not being available on GPU/TPU.\n",
        "fedjax.training.set_tf_cpu_only()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLz2tPAAUmtw"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "A federated algorithm trains a machine learning model over data distributed over several clients. At a high level, the server server randomly initializes the model parameters and other learning components. Then at each round it does the following:\n",
        "1. **Client selection**: Select a few clients at each round, typically at random.\n",
        "2. The server transmits the model parameters and other necessary components to the selected clients.\n",
        "3. **Client update**: The clients update the model parameter using a subroutine, which typically involves a few epochs of SGD.\n",
        "4. The clients transmit the updates to the server. \n",
        "5. **Server aggregation**: The server combines the updated model parameters and produces new model parameters. \n",
        "\n",
        "A pseudo code for a common federated learning algorithm can be found in Algorithm 1 in [Kairouz et al. (2020)](https://arxiv.org/pdf/1912.04977.pdf).\n",
        "Since FedJAX focuses on federated simulation and there is no actual transmission between clients and the server, we only focus on steps 1, 3, and 5, and ignore steps 2 and 4.\n",
        "Before we describe each of the modules, we will first describe how to use algorithms that are implemented in FedJAX. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdgSTuf5lmkl"
      },
      "source": [
        "# Algorithm overview\n",
        "\n",
        "We implement federated learning algorithms using the `FederatedAlgorithm` interface. The `FederatedAlgorithm` interface has two functions `init` and `apply`. Broadly our implementation has three parts.\n",
        "1. `ServerState`: This contains all the information available at the server at any given round. It includes model parameters and can also include other parameters that are used during optimization. At every round, a subset of `ServerState` is passed to the clients for federated learning. `ServerState` is also used in checkpointing and evaluation. Hence it is crucial that all the parameters that are modified during the course of the federated learning is stored as part of the `ServerState`. We recommend that `FederateAlgorithm` does not have any mutable parameters to ensure that it performs well with jax.\n",
        "\n",
        "2. `init`: Initializes the server state.\n",
        "\n",
        "3. `apply`: Takes a set of client_ids, corresponding datasets, and random keys together with `ServerState` and returns a new `ServerState` along with any information we need from the clients in the form of `client_diagnostics`.\n",
        "\n",
        "We demonstrate `FederatedAlgorithm` using the federated averraging implementation and emnist dataset. We first initialize the model, datasets and the federated algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISnsuB1w2yyV"
      },
      "outputs": [],
      "source": [
        "train, test = fedjax_data.emnist.load_data(only_digits=False,\n",
        "                                  mode='sqlite')\n",
        "model = fedjax.models.emnist.create_conv_model(only_digits=False)\n",
        "\n",
        "rng = random.PRNGKey(0)\n",
        "init_model_params = model.init(rng)\n",
        "# Federated algorithm requires a gradient function, a client optimizer, \n",
        "# a server optimizers, and parametes for batching at the client level.\n",
        "grad_fn = fedjax.model_grad(model)\n",
        "client_optimizer = fedjax.optimizers.sgd(0.1)\n",
        "server_optimizer = fedjax.optimizers.sgd(1.0)\n",
        "batch_hparams = fedjax.ShuffleRepeatBatchHParams(batch_size=10)\n",
        "fed_alg = fedjax.algorithms.fed_avg.federated_averaging(grad_fn,\n",
        "                                                        client_optimizer,\n",
        "                                                        server_optimizer,\n",
        "                                                        batch_hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msPo1V0S2zJn"
      },
      "source": [
        "We also note that similar to the rest of the library, we only pass on the necessary functions and parameters to the federated algorithm. Hence, to initialize the federated algorithm we only passed the `grad_fn` and did not pass the entire model. With this, we now initialize the server state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hcg4nrm3OkR"
      },
      "outputs": [],
      "source": [
        "init_server_state = fed_alg.init(init_model_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PZRj8yI3cvN"
      },
      "source": [
        "To run the federated algorithm, we pass the client data and server state to the `apply` function. For this end, we pass client data as a tuple of client id, client data, and the random keys. Adding client ids and random keys has multiple advantages. Firstly client client ids allows to track client diagnostics and would be helpful in debugging. Passing random keys would ensure deterministic execution and allow repeatability. Furthermore, as we discuss later it would help us with fast implementations. We first format the data in this necessary format and then run one round of federated learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCVLw_waljDZ"
      },
      "outputs": [],
      "source": [
        "# Select 10 client_ids and their data\n",
        "client_ids = list(train.client_ids())[0:5]\n",
        "clients_ids_and_data = list(train.get_clients(client_ids))\n",
        "\n",
        "client_inputs = []\n",
        "for i in range(5):\n",
        "  rng, use_rng = random.split(rng)\n",
        "  client_id, client_data = clients_ids_and_data[i]\n",
        "  client_inputs.append((client_id, client_data, use_rng))\n",
        "\n",
        "updated_server_state, client_diagnostics = fed_alg.apply(init_server_state,\n",
        "                                                         client_inputs)\n",
        "# Prints the l2 norm of gradients as part of client_diagnostics. \n",
        "print(client_diagnostics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS-g-97zlh1V"
      },
      "source": [
        "As we see above, the client statistics provide the `delta_l2_norm` of gradient for each client, which can be potentially used for debugging purposes. \n",
        "\n",
        "With this background on how to use existing implementations, we are now going to describe how to write your own federated algorithms in FedJAX. This involves three steps: client selection, client update, and server aggregation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_9flP9eW16y"
      },
      "source": [
        "# Client selection\n",
        "\n",
        "At each round of federated learning, typically clients are sampled uniformly at random. This can be done using numpy as follows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGlUf9oJUhrR"
      },
      "outputs": [],
      "source": [
        "all_client_ids = list(train.client_ids())\n",
        "print(\"Total number of client ids: \", len(all_client_ids))\n",
        "\n",
        "sampled_client_ids = np.random.choice(all_client_ids, size=2, replace=False)\n",
        "print(\"Sampled client ids: \", sampled_client_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cddaskha90o"
      },
      "source": [
        "However, the above code is not desirable due to the following reasons:\n",
        "1. For reproducibility, it is desirable to have a fixed seed just for sampling clients.\n",
        "2. Across rounds, different clients need to be sampled.\n",
        "3. For efficiency reasons, it might be better to do an approximately uniform sampling, where clients whose data is stored together are sampled together.\n",
        "4. Federated algorithms typically require additional randomness for batching, or dropout that needs to be sent to clients. \n",
        "\n",
        "To incorporate these features, we provide a few client samplers.\n",
        "1. UniformShuffledClientSampler\n",
        "2. UniformGetClientSampler\n",
        "\n",
        "UniformShuffledClientSampler is preferred for efficiency reasons, but if we need to sample clients truly randomly, UniformGetClientSampler can be used.\n",
        "Both of them return a list of client_ids, client_data, and client_rng."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFG74oD6cJXB"
      },
      "outputs": [],
      "source": [
        "efficient_sampler = fedjax.client_samplers.UniformShuffledClientSampler(\n",
        "    train.shuffled_clients(buffer_size=100), num_clients=2)\n",
        "print(\"Sampling from the efficient sampler.\")\n",
        "for round in range(0, 3):\n",
        "  sampled_clients_with_data = efficient_sampler.sample()\n",
        "  for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "    print(round, client_id)\n",
        "\n",
        "perfect_uniform_sampler = fedjax.client_samplers.UniformGetClientSampler(\n",
        "    train, num_clients=2, seed=1)\n",
        "print(\"Sampling from the perfect uniform sampler.\")\n",
        "for round in range(0, 3):\n",
        "  sampled_clients_with_data = perfect_uniform_sampler.sample()\n",
        "  for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "    print(round, client_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbINosSZg5KV"
      },
      "source": [
        " # Client update\n",
        "\n",
        "After selecting the clients, the next step would be running a model update step in the clients. Typically this is done running a few epochs of SGD. We only pass parts of the algorithm that are necessary for client update.\n",
        "\n",
        "The client update typically requires a set of parameters from the server (`init_params` in this example), the client dataset and a source of randomness (`rng`). The randomness can be used for dropout or other model update steps. Finally, instead of passing the entire model to the client update, since our code only depends on the gradient function, we pass `grad_fn` to client_update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M90B7IA2cYGQ"
      },
      "outputs": [],
      "source": [
        "def client_update(init_params, client_dataset, client_rng, grad_fn):\n",
        "  opt_state = client_optimizer.init(init_params)\n",
        "  params = init_params\n",
        "  for batch in client_dataset.shuffle_repeat_batch(batch_size=10):\n",
        "    client_rng, use_rng = random.split(client_rng)\n",
        "    grads = grad_fn(params, batch, use_rng)\n",
        "    opt_state, params = client_optimizer.apply(grads, opt_state, params)\n",
        "  delta_params = tree_util.tree_multimap(lambda a, b: a - b,\n",
        "                                              init_params, params)\n",
        "  return delta_params, len(client_dataset)\n",
        "\n",
        "client_sampler = fedjax.client_samplers.UniformGetClientSampler(\n",
        "    train, num_clients=2, seed=1)\n",
        "sampled_clients_with_data = perfect_uniform_sampler.sample()\n",
        "for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "  delta_params, num_samples = client_update(init_params,client_data, \n",
        "                                            client_rng, grad_fn)\n",
        "  print(client_id, num_samples, delta_params.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQtkrIz_QxiN"
      },
      "source": [
        "# Server aggregation \n",
        "\n",
        "The outputs of the clients are typically aggregated by computing the mean. This can be easily done by using the `tree_mean` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovrnJuYfFCEh"
      },
      "outputs": [],
      "source": [
        "sampled_clients_with_data = perfect_uniform_sampler.sample()\n",
        "client_updates = []\n",
        "for client_id, client_data, client_rng in sampled_clients_with_data:\n",
        "  delta_params, num_samples = client_update(init_params, client_data,\n",
        "                                            client_rng, grad_fn)\n",
        "  client_updates.append((delta_params, num_samples))\n",
        "updated_output = fedjax.tree_util.tree_mean(client_updates)\n",
        "print(updated_output.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1ch5Awyyz1"
      },
      "source": [
        "Combing the above steps gives federated algorithm, which can be found in the [example implementation of federated averaging.](https://github.com/google/fedjax/blob/main/examples/fed_avg.py).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiAi5qPazCC4"
      },
      "source": [
        "# Efficient implementation\n",
        "\n",
        "The above implementation would be efficient for running on single machines. JAX provides primitives such as `pmap` and `vmap` for efficient parallelization across many accelerators. We provide support for them in federated learning by distributing client computation across several accelerators.\n",
        "\n",
        "To take advantage of the faster implementation, one needs to implement `client_update` in a specific format. It has three functions `client_init`, `client_step`, and `client_final`, which we describe below.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mErDMSmAzApy"
      },
      "source": [
        " ## `client_init`\n",
        "\n",
        "This function takes the inputs from the server and outputs a `client_step_state` which will be passed in between rounds. It is desirable for the `client_ste_state` to be a dictionary. In this example, it just copies the parameters, optimizer_state and the current state of client randomness. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrNr5AR9_Bcw"
      },
      "outputs": [],
      "source": [
        "def client_init(server_params, client_rng):\n",
        "  opt_state = client_optimizer.init(server_params)\n",
        "  client_step_state = {\n",
        "      'params': server_params,\n",
        "      'opt_state': opt_state,\n",
        "      'rng': client_rng,\n",
        "  }\n",
        "  return client_step_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANU0_n05nERi"
      },
      "source": [
        "## `client_step`\n",
        "\n",
        "`client_step` takes the current step state and a batch of examples and updates the client_step_state. In this example, we run one step of SGD using the batch of examples and update client_step_state to reflect the new parameters, optimization state and randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKoJEj2_Dzl"
      },
      "outputs": [],
      "source": [
        "def client_step(client_step_state, batch):\n",
        "  rng, use_rng = random.split(client_step_state['rng'])\n",
        "  grads = grad_fn(client_step_state['params'], batch, use_rng)\n",
        "  opt_state, params = client_optimizer.apply(grads,\n",
        "                                              client_step_state['opt_state'],\n",
        "                                              client_step_state['params'])\n",
        "  next_client_step_state = {\n",
        "      'params': params,\n",
        "      'opt_state': opt_state,\n",
        "      'rng': rng,\n",
        "  }\n",
        "  return next_client_step_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJxUD3c9nloT"
      },
      "source": [
        "## `client_final`\n",
        "\n",
        "`client_final` modifies the final_client_step_state and returns the desired parameter. In this example, we compute the difference between initial parameters and the final updated parameters in the `client_final` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU0C7-Hy_Hfb"
      },
      "outputs": [],
      "source": [
        "def client_final(server_params, client_step_state):\n",
        "  delta_params = tree_util.tree_multimap(lambda a, b: a - b,\n",
        "                                         server_params,\n",
        "                                         client_step_state['params'])\n",
        "  return delta_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIS7GcMO_BV-"
      },
      "source": [
        "## `for_each_client`\n",
        "\n",
        "Once we have the three components, we can combine them to create a client_update function using the `for_each_client` function. `for_each_client` returns a function that can be used to run client updates. The sample usage is below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Feom9LGJohKv"
      },
      "outputs": [],
      "source": [
        "for_each_client_update = fedjax.for_each_client(client_init,\n",
        "                                                client_step,\n",
        "                                                client_final)\n",
        "\n",
        "client_sampler = fedjax.client_samplers.UniformGetClientSampler(\n",
        "    train, num_clients=2, seed=1)\n",
        "sampled_clients_with_data = perfect_uniform_sampler.sample()\n",
        "batched_clients_data = [\n",
        "      (cid, cds.shuffle_repeat_batch(batch_size=10), crng)\n",
        "      for cid, cds, crng in sampled_clients_with_data\n",
        "  ]\n",
        "for client_id, delta_params in for_each_client_update(init_params,\n",
        "                                                      batched_clients_data):\n",
        "  print(client_id, delta_params.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grLwbDGleeSt"
      },
      "source": [
        "We note that`for_each_client_update` requires the client data to be already batched. This is necessary for performance gains while using multiple accelerators. Furthermore, the batch size needs to be the same across all clients.\n",
        "\n",
        "By default `for_each_client` selects the standard JIT backend. To enable parallelism with TPUs or for debugging one can set it using `fedjax.set_for_each_client_backend(backend)`, where `backend` is either of 'pmap' or 'debug'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDtj-YRmooat"
      },
      "source": [
        "For each client function can also be used to add some additional step wise results, which can be used for debugging. This requires changing `client_step` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY5Rbf-7qam0"
      },
      "outputs": [],
      "source": [
        "def client_step_with_log(client_step_state, batch):\n",
        "  rng, use_rng = random.split(client_step_state['rng'])\n",
        "  grads = grad_fn(client_step_state['params'], batch, use_rng)\n",
        "  opt_state, params = client_optimizer.apply(grads,\n",
        "                                              client_step_state['opt_state'],\n",
        "                                              client_step_state['params'])\n",
        "  next_client_step_state = {\n",
        "      'params': params,\n",
        "      'opt_state': opt_state,\n",
        "      'rng': rng,\n",
        "  }\n",
        "  grad_norm = fedjax.tree_util.tree_l2_norm(grads)\n",
        "  return next_client_step_state, grad_norm\n",
        "\n",
        "\n",
        "for_each_client_update = fedjax.for_each_client(client_init,\n",
        "                                                      client_step_with_log,\n",
        "                                                      client_final,\n",
        "                                                      with_step_result=True)\n",
        "\n",
        "\n",
        "for client_id, delta_params, grad_norms in for_each_client_update(init_params,\n",
        "                                                                  batched_clients_data):\n",
        "  \n",
        "  print(client_id, list(delta_params.keys()))\n",
        "  print(client_id, np.array(grad_norms))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7vhcgT71B76"
      },
      "source": [
        "# Recap\n",
        "\n",
        "In this tutorial, we have covered the following:\n",
        "\n",
        "-   Using exisiting algorithms.\n",
        "-   Adding new algorithms.\n",
        "-   Efficient implementation in the presence of accelerators."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "algorithms_tutorial.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1oH2gnJgRcYB6icAzhdmUYaLQNgBOEj_m",
          "timestamp": 1623863953579
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
