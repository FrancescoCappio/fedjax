{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix7n_mL1BfQs"
      },
      "source": [
        "# Working with models in FedJAX.\n",
        "\n",
        "This chapter introduces `fedjax.Model`. We assume you already have finished the \"Datasets\" chapter. We first overview centralized training and evaluation with `fedjax.Model` and then describe how to add new neural architectures and specify additional evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyKDEkdrAMCL"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "import immutabledict\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import stax\n",
        "\n",
        "import fedjax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJp5GU6t032"
      },
      "source": [
        "\n",
        "## Centralized training \u0026 evaluation with `fedjax.Model`\n",
        "\n",
        "Most federated learning algorithms are built upon common components from standard centralized learning. `fedjax.Model` holds these common components. In centralized learning, we are mostly concerned with two tasks,\n",
        "\n",
        "-   Training: We want to optimize our model parameters on the training dataset.\n",
        "-   Evaluation: We want to know the values of evaluation metrics (e.g. accuracy) of the current model parameters on a test dataset.\n",
        "\n",
        "Let's first see how we can carry out these two tasks on the [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset) dataset with `fedjax.Model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zkLV40QuiFC"
      },
      "outputs": [],
      "source": [
        "# Load train/test splits of the EMNIST dataset.\n",
        "train, test = fedjax.datasets.emnist.load_data()\n",
        "\n",
        "# As a start, let's simply use a logistic regression model.\n",
        "model = fedjax.models.emnist.create_logistic_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNJVRZ5Nco5w"
      },
      "source": [
        "### Random initialization, the JAX way\n",
        "\n",
        "To start training, we need some randomly initialized parameters. In JAX, pseudo random number generation works slightly differently. For now, it is sufficient to know we call `jax.random.PRNGKey()` to seed the random number generator. JAX has a [detailed introduction](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) on this topic, if you are interested.\n",
        "\n",
        "To create the initial model parameters, we simply call `fedjax.Model.init()` with a `PRNGKey`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWt_MHELREhq"
      },
      "outputs": [],
      "source": [
        "params_rng = jax.random.PRNGKey(0)\n",
        "params = model.init(params_rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV53yxngfUxs"
      },
      "source": [
        "Here are our initial model parameters. With the same `PRNGKey`, we will always get the same random initialization. There are 2 parameters in our model, the weights `w`, and the bias `b`. They are organized into a `FlapMapping`, but in general any [PyTree](https://jax.readthedocs.io/en/latest/pytrees.html) can be used to store model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrf3dsFDfMiQ"
      },
      "outputs": [],
      "source": [
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIP-U4Q6ep6g"
      },
      "source": [
        "### Evaluating model parameters\n",
        "\n",
        "Before we start training, let's first see how our initial parameters fare on the train and test sets. Unsurprisingly, they do not do very well. We evaluate using the `fedjax.evaluate_model()` which takes in model, parameters, and datasets which are batched. As noted in the dataset tutorial, we batch using\n",
        "`fedjax.padded_batch_federated_data()` for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJwZvLswgjTg"
      },
      "outputs": [],
      "source": [
        "# We select first 16 batches using itertools.islice.\n",
        "batched_test_data = list(itertools.islice(\n",
        "    fedjax.padded_batch_federated_data(test, batch_size=128), 16))\n",
        "batched_train_data = list(itertools.islice(\n",
        "    fedjax.padded_batch_federated_data(train, batch_size=128), 16))\n",
        "\n",
        "print('eval_test', fedjax.evaluate_model(model, params, batched_test_data))\n",
        "print('eval_train', fedjax.evaluate_model(model, params, batched_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxro4lXqiDk1"
      },
      "source": [
        "How does our model know what evaluation metrics to report? It is simply specified in the `eval_metrics` field. We will discuss evaluation metrics in more detail later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdypCf-HineS"
      },
      "outputs": [],
      "source": [
        "model.eval_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av2qThtek5eU"
      },
      "source": [
        "Since `fedjax.evaluate_model` simply takes a stream of batches, we can also use it to evaluate multiple clients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlwA9v2flAMV"
      },
      "outputs": [],
      "source": [
        "for client_id, dataset in itertools.islice(test.clients(), 4):\n",
        "  print(\n",
        "      client_id,\n",
        "      fedjax.evaluate_model(model, params,\n",
        "                            dataset.padded_batch(batch_size=128)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyvylBo10BQe"
      },
      "source": [
        "### The training objective\n",
        "\n",
        "To train our model, we need two things: the objective function to minimize, and an optimizer.\n",
        "\n",
        "`fedjax.Model` contains two functions `apply_for_train()` and `train_loss()` that can be used to arrive at the training objective.\n",
        "\n",
        "-   `apply_for_train(params, batch_example, rng)` takes the current model parameters, a batch of examples, and a `PRNGKey`, and returns some output.\n",
        "-   `train_loss(batch_example, train_output)` translates the output of `apply_for_train()` into a vector of per-example loss values.\n",
        "\n",
        "In our example model, `apply_for_train()` produces a score for each class, and `train_loss()` is simply the cross entropy loss. `apply_for_train()` in this case does not make use of a `PRNGKey`, so we can pass `None` instead for convenience. A different `apply_for_train()` might actually make use of the `PRNGKey`, for tasks such as dropout. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM6Kgcgx23nN"
      },
      "outputs": [],
      "source": [
        "# train_batches is an infinite stream of shuffled batches of examples.\n",
        "def train_batches():\n",
        "  yield from fedjax.shuffle_repeat_batch_federated_data(\n",
        "      train,\n",
        "      batch_size=8,\n",
        "      client_buffer_size=16,\n",
        "      example_buffer_size=1024,\n",
        "      seed=0)\n",
        "\n",
        "# We obtain the first batch by using the `next` function.\n",
        "example = next(train_batches())\n",
        "output = model.apply_for_train(params, example, None)\n",
        "per_example_loss = model.train_loss(example, output)\n",
        "\n",
        "output.shape, per_example_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_SeJCPV495_"
      },
      "source": [
        "Note that the `output` is per example predictions and has shape (8, 62), where 8 is the batch size and 62 is the number of classes. Alternatively, we can use `model_per_example_loss()` to get a function that gives us the same result. `model_per_example_loss()` is a convenience function that does exactly what we just did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZerxAt2E5E51"
      },
      "outputs": [],
      "source": [
        "per_example_loss_fn = fedjax.model_per_example_loss(model)\n",
        "per_example_loss_fn(params, example, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMZOJ5jf7qUI"
      },
      "source": [
        "The training objective is a scalar, so why does `train_loss` return a vector of per-example loss values? First of all, the training objective in most cases is just the average of the per-example loss values, so arriving at the final training objective isn't hard. Moreover, in certain algorithms, we not only use the train loss over a single batch of examples for a stochastic training step, but also need to estimate the average train loss over an entire (client) dataset. Having the per-example loss values there is instrumental in obtaining the correct estimate when the batch sizes may vary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFJJLPd-84hV"
      },
      "outputs": [],
      "source": [
        "def train_objective(params, example):\n",
        "  return jnp.mean(per_example_loss_fn(params, example, None))\n",
        "\n",
        "train_objective(params, example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iZ2Gyi5ivfL"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "With the training objective at hand, we just need an optimizer to find some good model parameters that minimize it.\n",
        "\n",
        "There are many optimizer implementations in JAX out there, but fedjax doesn't force a choice over any. Instead, we provide a simple `fedjax.Optimizer` interface so a new optimizer implementation can be wrapped. For convenience, we provide some common optimizers wrapped from [optax](https://github.com/deepmind/optax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWt8diTVire5"
      },
      "outputs": [],
      "source": [
        "optimizer = fedjax.optimizers.adam(1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye3zAOiPkAJW"
      },
      "source": [
        "An optimizer is simply a pair of two functions, `init()` and `apply()`.\n",
        "\n",
        "-   `init(params)` returns the initial optimizer state, such as initial values for accumulators of gradients.\n",
        "-   `apply(grads, opt_state, params)` applies the gradients to update the current optimizer state and model parameters.\n",
        "\n",
        "Instead of modifying `opt_state` or `params`, `apply` returns a new pair of optimizer state and model parameters. In JAX, it is common to express computation in this stateless/mutation free style, often referred to as [functional programming](https://jax.readthedocs.io/en/latest/glossary.html#term-functional-programming), or [pure functions](https://jax.readthedocs.io/en/latest/glossary.html#term-pure-function). The pureness of functions is crucial to many features in JAX, and it is thus always good practice to write functions that do not modify its inputs. You have probably also noticed that all the functions of a `fedjax.Model` we have seen so far do not modify the model object itself (for example, `init()` returns model parameters instead of setting some attribute of `model`; `apply_for_train()` takes model parameters as an input argument, instead of getting it from `model`). We do so for exactly the purpose of keeping all functions pure.\n",
        "\n",
        "However, in the top level training loop, it is fine to mutate states since we are not in a function that may be transformed by JAX. Let's run our first training step, which resulted in a slight decrease in objective on the same batch of examples.\n",
        "\n",
        "To obtain the gradients, we use `jax.grad()` which returns the gradient function. More details about `jax.grad()` can be found from the [jax documentation](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQqZL48H9hLS"
      },
      "outputs": [],
      "source": [
        "opt_state = optimizer.init(params)\n",
        "grads = jax.grad(train_objective)(params, example)\n",
        "opt_state, params = optimizer.apply(grads, opt_state, params)\n",
        "train_objective(params, example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-UhOMAvcq3b"
      },
      "source": [
        "Instead of using `jax.grad()` directly, we also provide a convinient `fedjax.model_grad()` which computes the gradient of a model with respect to the averaged `fedjax.model_per_example_loss()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTBq-bNMcqM-"
      },
      "outputs": [],
      "source": [
        "model_grads = fedjax.model_grad(model)(params, example, None)\n",
        "opt_state, params = optimizer.apply(grads, opt_state, params)\n",
        "train_objective(params, example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ticoulv4_C1u"
      },
      "source": [
        "Let's wrap everything into a single JIT compiled function and train a few more steps, and evaluate again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUKwW0zs-yLG"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(example, opt_state, params):\n",
        "  grads = jax.grad(train_objective)(params, example)\n",
        "  return optimizer.apply(grads, opt_state, params)\n",
        "\n",
        "for example in itertools.islice(train_batches(), 5000):\n",
        "  opt_state, params = train_step(example, opt_state, params)\n",
        "\n",
        "print('eval_test', fedjax.evaluate_model(model, params, batched_test_data))\n",
        "print('eval_train', fedjax.evaluate_model(model, params, batched_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtjGbx-jCejU"
      },
      "source": [
        "## Building a custom model\n",
        "\n",
        "`fedjax.Model` has been built with customization in mind. We have already seen how to switch to a different training loss. In this section, we will discuss how the rest of a `fedjax.Model` can be customized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js0vCut_dx7B"
      },
      "source": [
        "### Training loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LGUk6_a5P7d"
      },
      "source": [
        "Because `train_loss()` is separate from `apply_for_train()`, it is easy to switch to a different loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq6EXir-6DZ4"
      },
      "outputs": [],
      "source": [
        "def hinge_loss(example, output):\n",
        "  label = example['y']\n",
        "  num_classes = output.shape[-1]\n",
        "  mask = jax.nn.one_hot(label, num_classes)\n",
        "  label_score = jnp.sum(output * mask, axis=-1)\n",
        "  best_score = jnp.max(output + 1 - mask, axis=-1)\n",
        "  return best_score - label_score\n",
        "\n",
        "\n",
        "hinge_model = model.replace(train_loss=hinge_loss)\n",
        "fedjax.model_per_example_loss(hinge_model)(params, example, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89IVhp9FKo09"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "We have already seen that the `eval_metrics` field of a `fedjax.Model` tells the model what metrics to evaluate. `eval_metrics` is an immutable mapping from names of the metric, to `fedjax.metrics.Metric` objects. A `fedjax.metrics.Metric` object tells us how to calculate a metric's value from multiple batches of examples. Like `fedjax.Model`, a `fedjax.metrics.Metric` is stateless.\n",
        "\n",
        "To customize the metrics to evaluate on, or what names to give to each, simply specify a different mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeBMvbyNCdyL"
      },
      "outputs": [],
      "source": [
        "only_accuracy = model.replace(\n",
        "    eval_metrics=immutabledict.immutabledict(\n",
        "        {'accuracy': fedjax.metrics.Accuracy()}))\n",
        "fedjax.evaluate_model(only_accuracy, params, batched_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8EBiEBPQxz6"
      },
      "source": [
        "There are already some concrete `Metric`s in `fedjax.metrics`. It is also easy to implement a new one. You can read more about how to implement a `Metric` in [its own introduction](https://fedjax.readthedocs.io/en/latest/fedjax.metrics.html).\n",
        "\n",
        "The bit of `fedjax.Model` that is directly relevant to evaluation is `apply_for_eval()`. The relation between `apply_for_eval()` and an evaluation metric is similar to that between `apply_for_train()` and `train_loss`: `apply_for_eval(params, example)` takes the model parameters and a batch of examples (notice there is no randomness in evaluation and thus we don't need a `PRNGKey`), and produces some prediction that evaluation metrics can consume. In the case of our example, the output from `apply_for_eval()` and `apply_for_train()` are identical, but they don't have to be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWWsQw8-Tu6s"
      },
      "outputs": [],
      "source": [
        "jnp.all(\n",
        "    model.apply_for_train(params, example, None) == model.apply_for_eval(\n",
        "        params, example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMt-xoqoUFVS"
      },
      "source": [
        "What `apply_for_eval` needs to produce really just depends on what evaluation `Metric`s will be used. In our case, we are using `fedjax.metrics.Accuracy`, and `fedjax.metrics.CrossEntropyLoss`. They are similar in their requirements on the inputs:\n",
        "\n",
        "-   They both need to know the true label from the `example`, using a `target_key` that defaults to `\"y\"`.\n",
        "-   They both need to know the predicted scores from `apply_for_eval`, customizable as `pred_key`. If `pred_key` is None, `apply_for_eval` should return just a vector of per-class scores; otherwise `pred_key` can be a string key, and `apply_for_eval` should return a mapping (e.g. `dict`) that maps the key to a vector of per-class scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nJPFkozVb1S"
      },
      "outputs": [],
      "source": [
        "fedjax.metrics.Accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWbn6osiVl9O"
      },
      "source": [
        "### Neural network architectures\n",
        "\n",
        "We have now covered all five parts of a `fedjax.Model`, namely `init`, `apply_for_train`, `apply_for_eval`, `train_loss`, and `eval_metrics`. `train_loss` and `eval_metrics` are easy to customize thanks for them being mostly agnostic to the actual neural network architecture of the model. `init`, `apply_for_train`, and `apply_for_eval` on the other hand, are closely related.\n",
        "\n",
        "In principle, as long as the three functions meet the interface we have seen so far, they can be used to build a custom model. Let's try to build a model that uses multi-layer perceptron that uses hinge loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc58iPxzWkHV"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(example, output):\n",
        "  label = example['y']\n",
        "  num_classes = output.shape[-1]\n",
        "  mask = jax.nn.one_hot(label, num_classes)\n",
        "  return -jnp.sum(jax.nn.log_softmax(output) * mask, axis=-1)\n",
        "\n",
        "def mlp_model(num_input_units, num_units, num_classes):\n",
        "\n",
        "  def mlp_init(rng):\n",
        "    w0_rng, w1_rng = jax.random.split(rng)\n",
        "    w0 = jax.random.uniform(w0_rng, [num_input_units, num_units])\n",
        "    b0 = jnp.zeros([num_units])\n",
        "    w1 = jax.random.uniform(w1_rng, [num_units, num_classes])\n",
        "    b1 = jnp.zeros([num_classes])\n",
        "    return w0, b0, w1, b1\n",
        "\n",
        "  def mlp_apply(params, batch, rng=None):\n",
        "    w0, b0, w1, b1 = params\n",
        "    x = batch['x']\n",
        "    batch_size = x.shape[0]\n",
        "    h = jax.nn.relu(x.reshape([batch_size, -1]) @ w0 + b0)\n",
        "    return h @ w1 + b1\n",
        "\n",
        "  return fedjax.Model(\n",
        "      init=mlp_init,\n",
        "      apply_for_train=mlp_apply,\n",
        "      apply_for_eval=mlp_apply,\n",
        "      train_loss=cross_entropy_loss,\n",
        "      eval_metrics={'accuracy': fedjax.metrics.Accuracy()})\n",
        "\n",
        "\n",
        "# There are 28*28 input pixels, and 62 classes in EMNIST.\n",
        "mlp = mlp_model(28 * 28, 128, 62)\n",
        "\n",
        "@jax.jit\n",
        "def mlp_train_step(example, opt_state, params):\n",
        "\n",
        "  @jax.grad\n",
        "  def grad_fn(params, example):\n",
        "    return jnp.mean(fedjax.model_per_example_loss(mlp)(params, example, None))\n",
        "\n",
        "  grads = grad_fn(params, example)\n",
        "  return optimizer.apply(grads, opt_state, params)\n",
        "\n",
        "\n",
        "params = mlp.init(jax.random.PRNGKey(0))\n",
        "opt_state = optimizer.init(params)\n",
        "print('eval_test before training:',\n",
        "      fedjax.evaluate_model(mlp, params, batched_test_data))\n",
        "for example in itertools.islice(train_batches(), 5000):\n",
        "  opt_state, params = mlp_train_step(example, opt_state, params)\n",
        "print('eval_test after training:',\n",
        "      fedjax.evaluate_model(mlp, params, batched_test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFdiYhzlboVB"
      },
      "source": [
        "While writing custom neural network architecture from scratch is possible, most of the time, it is much more convenient to use a neural network library such as [Haiku](https://github.com/deepmind/dm-haiku) or [`jax.experimental.stax`](https://jax.readthedocs.io/en/latest/jax.experimental.stax.html). The two functions `fedjax.create_model_from_haiku` and `fedjax.create_model_from_stax` can convert a neural network expressed in the respective framework into a `fedjax.Model`. Let's build a convolutional network using `jax.experimental.stax` this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22Ls5ZFhcXFO"
      },
      "outputs": [],
      "source": [
        "def stax_mlp_model(input_shape, num_classes):\n",
        "  stax_init, stax_apply = stax.serial(\n",
        "      stax.Conv(\n",
        "          out_chan=64, filter_shape=(3, 3), strides=(1, 1), padding='SAME'),\n",
        "      stax.Relu,\n",
        "      stax.Flatten,\n",
        "      stax.Dense(256),\n",
        "      stax.Relu,\n",
        "      stax.Dense(num_classes),\n",
        "  )\n",
        "  return fedjax.create_model_from_stax(\n",
        "      stax_init=stax_init,\n",
        "      stax_apply=stax_apply,\n",
        "      sample_shape=input_shape,\n",
        "      train_loss=cross_entropy_loss,\n",
        "      eval_metrics={'accuracy': fedjax.metrics.Accuracy()})\n",
        "\n",
        "\n",
        "stax_mlp = stax_mlp_model([-1, 28, 28, 1], 62)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def stax_mlp_train_step(example, opt_state, params):\n",
        "\n",
        "  @jax.grad\n",
        "  def grad_fn(params, example):\n",
        "    return jnp.mean(\n",
        "        fedjax.model_per_example_loss(stax_mlp)(params, example, None))\n",
        "\n",
        "  grads = grad_fn(params, example)\n",
        "  return optimizer.apply(grads, opt_state, params)\n",
        "\n",
        "\n",
        "params = stax_mlp.init(jax.random.PRNGKey(0))\n",
        "opt_state = optimizer.init(params)\n",
        "print('eval_test before training:',\n",
        "      fedjax.evaluate_model(stax_mlp, params, batched_test_data))\n",
        "for example in itertools.islice(train_batches(), 1000):\n",
        "  opt_state, params = stax_mlp_train_step(example, opt_state, params)\n",
        "print('eval_test after training:',\n",
        "      fedjax.evaluate_model(stax_mlp, params, batched_test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1m1Qn_Cocn"
      },
      "source": [
        "## Recap\n",
        "\n",
        "In this chapter, we have covered the following:\n",
        "\n",
        "-   Components of `fedjax.Model`: `init()`, `apply_for_train()`, `apply_for_eval()`, `train_loss()`, and `eval_metrics`.\n",
        "-   Optimizers\n",
        "-   Standard centralized learning with a `fedjax.Model`.\n",
        "-   Specifying evaluation metircs.\n",
        "-   Building a custom `fedjax.Model`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Copy of model_tutorial.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1RyJCDzMv3qs-3DsCTDcb--_UDgUz9gG3",
          "timestamp": 1623874921648
        },
        {
          "file_id": "1Aojsnaxcom8pEKme6JFDuDklPwjNDzUE",
          "timestamp": 1622755811665
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
